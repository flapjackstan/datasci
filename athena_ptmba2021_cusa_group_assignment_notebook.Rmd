---
title: "GPA Analytics - The Market for Microvans"
output:
  html_notebook: default
  pdf_document: default
  html_document:
    df_print: paged
---
```{r}
library(tidyr)
library(dendextend)
library(corrplot)
library(data.table)
library(dbplyr)
library(forcats)
library(gclus)
library(ggplot2)
library(olsrr)
library(psych)
library(purrr)
library(readr)
library(stringr)
library(tibble)
library(tidyverse)
```
This notebook contains all the analysis done by the Athena group in the context of the group assignment for the Customer Analytics course. 

# Background Information
GPA is a consulting company focused on the automotive market in the US. Recent data shows a new trend around purchasing interest for a new category of cars, the so called *microvans*. Considering the expertise of the company, they started to investigate further the drivers of such a trend and what customers would expect from the new category. 

To speed up the process without loss of quality in the data, the company included concept-relevant questions to their existing panel and ensured that the data used for the investigation was only representative of households with a non-zero likelihood of purchasing a car within the next 12 months. A sample of this data is the one used over the course of the analysis we will be describing in this document.

***

## Glossary

### Survey Variables

Variable ID | Variable Name | Variable Definition
------------|---------------|-----------------------
v01         |kidtrans       |We need a car that helps transport our kids and their friends.
v02         |miniboxy       |Current minivans are simply too boxy and large.
v03         |lthrbetrv      |Leather seats are dramatically better than cloth.
v04         |secbiggr       |If we got a second car, it would need to be bigger than a standard sedan.
v05         |safeimpt       |Auto safety is very important to me.
v06         |buyhghnd       |We tend to buy higher‐end cars.
v07         |pricqual       |Car prices strongly reflect underlying production quality.
v08         |prmsound       |A premium sound and entertainment system helps on long car trips.
v09         |perfimpt       |Performance is very important in a car.
v10         |tkvacatn      |We try to take as many vacations as possible.
v11         |noparkrm       |Our current residence doesn't have a lot of parking room.
v12         |homlrgst       |Our home is among the largest in the neighborhood.
v13         |envrminr       |The environmental impact of automobiles is relatively minor. 
v14         |needbetw       |There needs to be something between a sedan and a minivan. 
v15         |suvcmpct       |I like SUVs more than minivans since they're more compact.
v16         |next2str       |My next car will be a two‐seater.
v17         |carefmny       |We are careful with money.
v18         |shdcarpl       |I think everyone should carpool or take public transportation
v19         |imprtapp       |Most of our appliances are imported.
v20         |lk4whldr       |Four‐wheel drive is a very attractive option.
v21         |kidsbulk       |Our kids tend to take a lot of bulky items and toys with them.
v22         |wntguzlr       |I will buy what I want even if it is a “gas guzzler”.
v23         |nordtrps       |We don’t go on road trips with the family.
v24         |stylclth       |We tend to purchase stylish clothes for the family.
v25         |strngwrn       |Warranty protection needs to be strong on a new car.
v26         |passnimp       |Passion for one’s job is more important than pay.
v27         |twoincom       |Our family would find it hard to subsist on just one income.
v28         |nohummer       |I am not interested in owning a vehicle like a Hummer.
v29         |aftrschl       |We engage in more after‐school activities than most families.
v30         |accesfun       |Accessories really make a car more fun to drive.

### Demographic Information

Demographic Information | Description
------------------------|------------
age|Age of respondent in years
income|Annual household income in thousands of dollars 
miles|Total annual amount driven by household members in thousands of miles
numkids|Number of children (aged 0‐18) residing in household 
female|Whether or not the respondent is a female
educ|Education level of respondent (1 = High School, 2 = Some College, 3 = Undergraduate Degree, 4 = Graduate Degree)
recycle|Self‐reported recycling compared to average (1 = Much Less, 2 = Somewhat Less, 3 = Average, 4 = Somewhat More, 5 = Much More)

### General Information

subjnumb | Subject ID
mvliking | Indicator of whether the subject likes or not the concept of a microvan

***
## Exploring the Data - Descriptive Statistics
```{r}
microvan <- read.csv("data/microvan.csv", sep=';')
microvan <- as.data.table(lapply(microvan, as.numeric))
str(microvan)
```
### Summary Statistics
```{r}
psych::describe(microvan)
```
### Histograms of the variables
```{r}
microvan %>%
  select(-"subjnumb") %>% 
  gather() %>% 
  ggplot(aes(value)) +
  facet_wrap(~ key, scales = "free") +
  geom_histogram()+
  theme_minimal()
```

### Scatter plots & Correlations
```{r}
pairs(microvan[,2:11], lower.panel = NULL) 
```
```{r}
cor <- cor(microvan[,2:32])
corr.matrix <- cor(microvan[ , 2:32])
upper<-round(corr.matrix,2)
upper[upper.tri(corr.matrix)]<-""
upper<-as.data.frame(upper)
upper
```
## Regression Analysis

### Traditional Regression
```{r}
regfit <- lm(mvliking ~ ., data = microvan[,2:32])
summary(regfit)
```

### Stepwise Regression
```{r}
ols_step_both_p(regfit, pent = 0.1, prem = 0.3, details=TRUE)
```


## Factor and Clustering
### Standardizing the data
```{r}
scaled_data <- scale(microvan[, 3:32])
```

### Calcuate the correlations of the scaled data
```{r}
cor_ <- cor(scaled_data)
```

### Exploring the Eigenvalues
```{r}
EV = eigen(cor_)$values
EV
```
### Looking into how much variance is explained by the Eigenvalues
```{r}
# Shares for the cumulative variance explained
plot(cumsum(EV/length(EV)), 
     type = "o", # type of plot: "o" for points and lines 'overplotted'
     col = "darkblue",
     pch = 16, # plot symbol: 16 = filled circle
     cex = 1, # size of plot symbols
     xlab = "Number of factors", # a title for the x axis
     ylab = "Cumulative variance explained", # a title for the y axis
     lwd = 2) # line width
abline(v = 5, lwd = 2, col = "grey") # draw a vertical line at v = 3
```
```{r}
cumsum(EV/length(EV))
```
It seems like the last big jump in incremental explainability happens for the 5th eigenvector, indicating that a PCA and Clustering with 5 could be benefitial.

### Run PCA in the dataset
```{r} 
EFA1 <- fa(r = cor_, 
           nfactors = 5, 
           fm = "pa", 
           rotate = "none")
print(EFA1, 
      digits = 3, # to round numbers to the third digit 
      cut = 0.35, # to show only values > 0.35
      sort = TRUE # to sort rows by loading size
)
```
Factors seem to have a too unclear definition, let's try a rotation to evaluate the results
```{r} 
EFA2 <- fa(r = cor_, 
           nfactors = 5, 
           fm = "pa", 
           rotate = "varimax")
print(EFA2, 
      digits = 3, # to round numbers to the third digit 
      cut = 0.35, # to show only values > 0.35
      sort = TRUE # to sort rows by loading size
)
```
Factor 1 seems to be related to luxury; Factor 2 related to size (something small but spacious); Factor 3 to families with kids; factor 4 to adventure takers; factor 5 environmentally conscious

```{r}
EFA2.scores = factor.scores(scaled_data, unclass(EFA2$loadings))$scores
head(EFA2.scores) # to show the first 6 observations
```
```{r}
microvan.scores <- cbind(microvan, EFA2.scores)
head(microvan.scores)
```
We can now build a new data frame where each observation only has the demographic information and the factors, as well as a new data frame of the 5 factors to use in the clustering.
```{r}
data_reduced <-data.frame(microvan.scores[,c(1,2,33:44)])
cluster_data <-data_reduced[,c(10:14)]

```
***
### Before we proceed with K-Means as a partitioning method, let's split the data into calibration and validation
#### Split the data intro calibration and validation
```{r}
fractionCalib   <- 0.80
fractionValidation <- 0.20
# Compute sample sizes.
sampleSizeCalib   <- floor(fractionCalib   * nrow(cluster_data))
sampleSizeValidation <- floor(fractionValidation * nrow(cluster_data))

# Create the randomly-sampled indices for the dataframe. Use setdiff() to
# avoid overlapping subsets of indices.
indicesCalib   <- sort(sample(seq_len(nrow(cluster_data)), size=sampleSizeCalib))
indicesNotCalib <- setdiff(seq_len(nrow(cluster_data)), indicesCalib)
indicesValidation  <- sort(sample(indicesNotCalib, size=sampleSizeValidation))

# Finally, output the three dataframes for training, validation and test.
dfCalib  <- cluster_data[indicesCalib, ]
dfValidation <- cluster_data[indicesValidation, ]
```

### Run a cluster analysis on a distance matrix and using the Ward method
```{r}
clust_ward <- hclust(dist(dfCalib), method="ward.D2") 
```

### Scree plot
```{r}
plot(rev(clust_ward$height), # rev is used to plot from low to high values on Y axis
     type = "b",           # to display both the points and lines
     ylab = "Dissimilarity measure",
     xlab = "Number of clusters",
     main = "Scree plot",
     col = "darkblue",
     pch = 16,
     xlim = c(1,15))             # specify the plot symbol: 16 = filled circle
abline(v = 4, lty = 2, col = "darkred") # draw a vertical line at v = 5 
```


### Dendrogram 
```{r}
library(dendextend)
plot(set(as.dendrogram(clust_ward),  
         "branches_k_color", # to highlight the cluster solution with a color
         k = 3),
     ylab = "Distance",
     main = "Dendrogram",
     cex = 0.2)             # Size of labels
```

From the Dendrogram and the Screeplot it seems like either 5 or 6 clusters could be good enough. From PCA and the Eigenvalues we can go with 5, which seems to be the common value.

```{r}
memb <- cutree(clust_ward, k = 4)
table(memb)
```

### K-Means Clustering


#### Calculate the centroids on the calibration data
```{r}
cent <- NULL
for(k in 1:3){
  cent <- rbind(cent, colMeans(dfCalib[memb == k, , drop = FALSE]))
}
round(cent, 3)
```

#### We do the clustering on the calibration data
```{r}
set.seed(1)
kmeans1 <- kmeans(dfCalib, centers = cent, iter.max = 10)
```

#### Now we do some crossvalidation with the validation data
```{r}
set.seed(1)
kmeans2 <- kmeans(dfValidation, centers = kmeans1$centers, iter.max = 1)
```
### And we store the assignments to a variable 
```{r}
dfValidation.vali1 <- cbind(dfValidation, KS1 = kmeans2$cluster)
```
#### Now let's do a full clustering
```{r}
kmeans3 <- kmeans(dfValidation, centers = cent, iter.max = 10)
dfValidation.vali2 <- cbind(dfValidation, KS2 = kmeans3$cluster)
```
#### Finally we crosstabulate
```{r}
dfValidation.vali1 <-as.data.frame(dfValidation.vali1)
dfValidation.vali2 <-as.data.frame(dfValidation.vali2)
table(KS1 = dfValidation.vali1$KS1, KS2 = dfValidation.vali2$KS2) 
```

From the crossvalidation it seems that the clustering is robust enough and there is no contradiction. That being the case, let's apply the clustering to the entire dataset and interpret it.

#### K-Means to the entire data
### Run a cluster analysis on a distance matrix and using the Ward method
```{r}
clust_ward_final <- hclust(dist(cluster_data), method="ward.D2") 
```

### Scree plot
```{r}
plot(rev(clust_ward_final$height), # rev is used to plot from low to high values on Y axis
     type = "b",           # to display both the points and lines
     ylab = "Dissimilarity measure",
     xlab = "Number of clusters",
     main = "Scree plot",
     col = "darkblue",
     pch = 16,
     xlim = c(1,15))             # specify the plot symbol: 16 = filled circle
abline(v = 4, lty = 2, col = "darkred") # draw a vertical line at v = 5 
```

### Dendrogram 
```{r}
library(dendextend)
plot(set(as.dendrogram(clust_ward_final),  
         "branches_k_color", # to highlight the cluster solution with a color
         k = 3),
     ylab = "Distance",
     main = "Dendrogram",
     cex = 0.2)             # Size of labels
```

```{r}
cent <- NULL
for(k in 1:3){
  cent <- rbind(cent, colMeans(cluster_data[memb == k, , drop = FALSE]))
}
round(cent, 3)
set.seed(1)
kmeans_tot <- kmeans(cluster_data, centers = cent, iter.max = 10)
data_reduced.clusters <- cbind(data_reduced, cluster = kmeans_tot$cluster)
```
```{r}
memb_final <- cutree(clust_ward_final, k = 3)
table(memb_final)
```
#### We begin to interpret by looking at the means
```{r}
aggregate(data_reduced.clusters,
          by = list(cluster = data_reduced.clusters$cluster), 
          FUN = mean)
```
